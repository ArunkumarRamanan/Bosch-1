{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert categorical to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "\n",
    "import c\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covert categorical data to numbers\n",
    "(Saves approx 25% memory usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375.940962076\n"
     ]
    }
   ],
   "source": [
    "# Read all data\n",
    "a=time.time()\n",
    "b=pd.read_csv(os.path.join(c.BASE_PATH,'test_categorical.csv'), dtype='object')\n",
    "print time.time()-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 500\n",
      "100.53592205\n",
      "0.201073518276\n",
      "500 1000\n",
      "105.001366854\n",
      "0.210008617878\n",
      "1000 1500\n",
      "100.97073698\n",
      "0.201954011917\n",
      "1500 2000\n",
      "100.874717951\n",
      "0.20175117588\n",
      "2000 2141\n",
      "27.5970749855\n",
      "0.0551967420578\n"
     ]
    }
   ],
   "source": [
    "# Batch wise stripping of elements\n",
    "i_s = [0, 500, 1000, 1500, 2000]\n",
    "i_e = [500, 1000, 1500, 2000, 2141]\n",
    "\n",
    "for s,e in zip(i_s, i_e):\n",
    "    print s, e\n",
    "    #a=time.time()\n",
    "    b.iloc[:,s:e]=b.iloc[:,s:e].apply(lambda x: x.str.lstrip('T'), axis=0)\n",
    "    #print (time.time()-a)\n",
    "    #print (time.time()-a)/500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b.to_csv(os.path.join(c.BASE_PATH,'test_categorical_to_num.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Convert to sparse and check memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def memory_usage(var):\n",
    "    ''' Returns estimate of memory usage using pickle trick '''\n",
    "    tmp_file_path = os.path.join(c.BASE_PATH, 'tmp_dump_file.pkl')\n",
    "    \n",
    "    with open(tmp_file_path,'wb') as f:\n",
    "        pickle.dump(var, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    file_size = os.path.getsize(tmp_file_path)/1024/1024\n",
    "    os.remove(tmp_file_path)\n",
    "    \n",
    "    return file_size\n",
    "\n",
    "def read_csv_as_sparse(csv_file, chuncksize=10000, verbose=False):\n",
    "    ''' Read csv and return sparse matrix'''\n",
    "    \n",
    "    print('Reading {} with size {} MB'.format(csv_file, os.path.getsize(csv_file)/1024/1024))\n",
    "    \n",
    "    # Check for columns to read\n",
    "    quick_scan = pd.read_csv(csv_file,\n",
    "                         nrows=1, \n",
    "                         dtype=np.float32,\n",
    "                         index_col = 0)\n",
    "    \n",
    "    if 'Response' in quick_scan.columns:\n",
    "        if verbose:\n",
    "            print('Excluding Response column')\n",
    "        columns_names_used = quick_scan.columns[0:quick_scan.shape[1]-1]\n",
    "    else:\n",
    "        columns_names_used = quick_scan.columns[0:quick_scan.shape[1]]\n",
    "    \n",
    "    \n",
    "    reader = pd.read_csv(csv_file,\n",
    "                         chunksize=chuncksize, \n",
    "                         dtype=np.float32,\n",
    "                         index_col=0,\n",
    "                         usecols=columns_names_used)\n",
    "    \n",
    "    \n",
    "    ids = pd.read_csv(csv_file, usecols=[0])\n",
    "    \n",
    "    if verbose:\n",
    "        for i,ch in enumerate(reader):\n",
    "            print ch.columns\n",
    "            \n",
    "            if i==0:\n",
    "                csr = csr_matrix(ch.fillna(0))\n",
    "            else:\n",
    "                csr = vstack([csr, csr_matrix(ch.fillna(0))], format='csr')\n",
    "\n",
    "            if not i % 10:\n",
    "                print('Doing chunck: {} | status: {} elements'.format(i, csr.getnnz()))\n",
    "    else:\n",
    "        csr = vstack([csr_matrix(ch.fillna(0)) for ch in reader], format='csr')\n",
    "    \n",
    "    print('Sparse matrix has {} elements and {} MB memory usage'.format(csr.getnnz(),\n",
    "                                                                       memory_usage(csr)))\n",
    "    \n",
    "    return csr, ids, columns_names_used\n",
    "\n",
    "def read_last_column(csv_file):\n",
    "    ''' Reads last column in csv file'''\n",
    "    sample = pd.read_csv(os.path.join(c.BASE_PATH,csv_file), nrows=1)\n",
    "    \n",
    "    return pd.read_csv(os.path.join(c.BASE_PATH,csv_file), usecols=[0,sample.shape[1]-1], index_col=0)\n",
    "\n",
    "def convert_data_file_to_pickle(filename, verbose=False):\n",
    "    ''' Reads full csv file, converts to sparse and stores using pickle with metadata'''\n",
    "    a = time.time()\n",
    "    csr, ids, f_names = read_csv_as_sparse(os.path.join(c.BASE_PATH, filename + '.csv'), \n",
    "                                  chuncksize=100000,\n",
    "                                  verbose=verbose)\n",
    "    print('Reading data as sparse took {}s'.format(time.time()-a))\n",
    "    \n",
    "    y = read_last_column(os.path.join(c.BASE_PATH, 'train_numeric.csv'))\n",
    "    \n",
    "    output = {'data': {'ids': ids, 'y': y, 'features': csr, 'feature_names': f_names},\n",
    "              'creation_date':datetime.datetime.now(),\n",
    "              'created_by': 'joostgp',\n",
    "              'script': 'convert_data_to_sparse'}\n",
    "    \n",
    "    save_path = os.path.join(c.BASE_PATH,filename + '.pkl')\n",
    "    \n",
    "    with open(save_path,'wb') as f:\n",
    "        pickle.dump(output, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    print('Results stored in {}'.format(save_path))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_files = ['train_categorical_to_num',\n",
    "              'train_numeric',\n",
    "              'train_date',\n",
    "              'test_categorical_to_num',\n",
    "              'test_numeric',\n",
    "              'test_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Volumes/My Book/kaggle_bosch/train_categorical_to_num.csv with size 2489 MB\n"
     ]
    }
   ],
   "source": [
    "for f in data_files:\n",
    "     convert_data_file_to_pickle(f, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Reading /Volumes/My Book/kaggle_bosch/train_categorical_to_num.csv with size 2489 MB\n",
    "Sparse matrix has 67650912 elements and 520 MB memory usage\n",
    "Reading data as sparse took 300.818804979s\n",
    "Results stored in /Volumes/My Book/kaggle_bosch/train_categorical_to_num.pkl\n",
    "Reading /Volumes/My Book/kaggle_bosch/train_numeric.csv with size 2040 MB\n",
    "Sparse matrix has 173670587 elements and 1329 MB memory usage\n",
    "Reading data as sparse took 139.659316063s\n",
    "Results stored in /Volumes/My Book/kaggle_bosch/train_numeric.pkl\n",
    "Reading /Volumes/My Book/kaggle_bosch/train_date.csv with size 2759 MB\n",
    "Sparse matrix has 242306507 elements and 1853 MB memory usage\n",
    "Reading data as sparse took 205.178919077s\n",
    "Results stored in /Volumes/My Book/kaggle_bosch/train_date.pkl\n",
    "Reading /Volumes/My Book/kaggle_bosch/test_categorical_to_num.csv with size 2489 MB\n",
    "Sparse matrix has 67615904 elements and 520 MB memory usage\n",
    "Reading data as sparse took 385.804188967s\n",
    "Results stored in /Volumes/My Book/kaggle_bosch/test_categorical_to_num.pkl\n",
    "Reading /Volumes/My Book/kaggle_bosch/test_numeric.csv with size 2038 MB\n",
    "Sparse matrix has 173621481 elements and 1329 MB memory usage\n",
    "Reading data as sparse took 138.873883963s\n",
    "Results stored in /Volumes/My Book/kaggle_bosch/test_numeric.pkl\n",
    "Reading /Volumes/My Book/kaggle_bosch/test_date.csv with size 2759 MB\n",
    "Sparse matrix has 242237132 elements and 1852 MB memory usage\n",
    "Reading data as sparse took 230.876074076s\n",
    "Results stored in /Volumes/My Book/kaggle_bosch/test_date.pkl\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
